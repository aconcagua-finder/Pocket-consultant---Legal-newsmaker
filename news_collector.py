#!/usr/bin/env python3
"""
News Collector –¥–ª—è NEWSMAKER

–ú–æ–¥—É–ª—å –¥–ª—è –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ —Å–±–æ—Ä–∞ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ Perplexity Deep Research
–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Ö –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –≤–∏–¥–µ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–π –ø—É–±–ª–∏–∫–∞—Ü–∏–∏.
"""

import json
import os
import asyncio
import requests  # –î–æ–±–∞–≤–ª–µ–Ω –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–π –∏–º–ø–æ—Ä—Ç
import time  # –î–æ–±–∞–≤–ª–µ–Ω –∏–º–ø–æ—Ä—Ç time –≤ –Ω–∞—á–∞–ª–æ
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, Dict, List
from loguru import logger

import config
from perplexity_client import PerplexityClient
from openai_client import OpenAIClient
from prompts import (
    get_perplexity_daily_collection_prompt,
    get_perplexity_system_prompt,
    parse_collected_news,
    PromptConfig
)
from retry_handler import retry_with_exponential_backoff, PerplexityRetryHandler
from cache_manager import cache_news_data, cache_api_response, cache_manager
from async_handler import batch_generate_images, run_async
from monitoring import monitor_performance, metrics_collector
from file_utils import safe_json_write, safe_json_read, create_backup, FileLock
from timezone_utils import now_msk, yesterday_msk, format_date_russian


class NewsCollector:
    """–ö–æ–ª–ª–µ–∫—Ç–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ —Å–±–æ—Ä–∞ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
    
    def __init__(self):
        self.perplexity_client = PerplexityClient()
        self.openai_client = OpenAIClient()
        self.data_dir = Path(config.DATA_DIR)
        self.data_dir.mkdir(exist_ok=True)
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        self.images_dir = self.data_dir / "images"
        self.images_dir.mkdir(exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º retry handler
        self.perplexity_retry = PerplexityRetryHandler()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Å–±–æ—Ä–∞
        self.max_retries = 3
        self.retry_delay = 60  # 1 –º–∏–Ω—É—Ç–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
    
    def _get_news_file_path(self, date: datetime) -> Path:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∞—Ç—ã
        
        Args:
            date: –î–∞—Ç–∞ –¥–ª—è —Ñ–∞–π–ª–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
            
        Returns:
            Path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
        """
        date_str = date.strftime('%Y-%m-%d')
        filename = config.NEWS_FILE_PATTERN.format(date=date_str)
        return self.data_dir / filename
    
    def _get_image_file_path(self, date: datetime, news_id: str) -> Path:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –Ω–æ–≤–æ—Å—Ç–∏
        
        Args:
            date: –î–∞—Ç–∞ –Ω–æ–≤–æ—Å—Ç–∏
            news_id: ID –Ω–æ–≤–æ—Å—Ç–∏
            
        Returns:
            Path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        """
        date_str = date.strftime('%Y-%m-%d')
        date_images_dir = self.images_dir / date_str
        date_images_dir.mkdir(exist_ok=True)
        return date_images_dir / f"{news_id}.png"
    
    @monitor_performance("image_generation_batch")
    async def _generate_images_async(self, news_list: List[Dict], target_date: datetime) -> List[Dict]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        
        Args:
            news_list: –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π
            target_date: –î–∞—Ç–∞ –¥–ª—è –Ω–∞–∑–≤–∞–Ω–∏—è –ø–∞–ø–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            
        Returns:
            List[Dict]: –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ–± –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö
        """
        logger.info("üé® –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –≤—Å–µ—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
        prompts = []
        for news_item in news_list:
            content = news_item.get('content', '')
            prompts.append(content)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ (–º–∞–∫—Å–∏–º—É–º 3 –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ)
        images = await batch_generate_images(prompts, max_concurrent=3)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –Ω–æ–≤–æ—Å—Ç–∏ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
        updated_news_list = []
        successful_images = 0
        
        for i, (news_item, image_bytes) in enumerate(zip(news_list, images), 1):
            news_id = news_item.get('id', f'news_{i}')
            title = news_item.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
            
            if image_bytes:
                try:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    image_path = self._get_image_file_path(target_date, news_id)
                    
                    with FileLock(image_path):
                        with open(image_path, 'wb') as f:
                            f.write(image_bytes)
                    
                    # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –¥–ª—è JSON
                    relative_image_path = str(image_path.relative_to(Path.cwd()))
                    
                    logger.info(f"‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {i}/{len(news_list)} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {image_path.name}")
                    
                    news_item.update({
                        'image_path': relative_image_path,
                        'image_generated': True,
                        'image_size': len(image_bytes)
                    })
                    successful_images += 1
                    
                    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫—É
                    metrics_collector.counters['image_generation_success'] += 1
                    
                except Exception as e:
                    logger.error(f"üí• –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {i}: {e}")
                    news_item.update({
                        'image_path': None,
                        'image_generated': False,
                        'image_error': str(e)
                    })
                    metrics_collector.counters['image_generation_failed'] += 1
            else:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {i}")
                news_item.update({
                    'image_path': None,
                    'image_generated': False,
                    'image_error': '–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å'
                })
                metrics_collector.counters['image_generation_failed'] += 1
            
            updated_news_list.append(news_item)
        
        logger.info(f"üéâ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {successful_images}/{len(news_list)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —É—Å–ø–µ—à–Ω–æ")
        
        return updated_news_list
    
    def _generate_images_for_news(self, news_list: List[Dict], target_date: datetime) -> List[Dict]:
        """
        Wrapper –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ - –≤—ã–∑—ã–≤–∞–µ—Ç –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
        """
        return run_async(self._generate_images_async(news_list, target_date))
    
    def _cleanup_old_files(self):
        """–£–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ —Ñ–∞–π–ª—ã –Ω–æ–≤–æ—Å—Ç–µ–π –∏ –æ—á–∏—â–∞–µ—Ç –∫–µ—à"""
        try:
            cutoff_date = now_msk() - timedelta(days=config.MAX_NEWS_FILES)
            
            for file_path in self.data_dir.glob("daily_news_*.json"):
                try:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                    date_part = file_path.stem.replace('daily_news_', '')
                    file_date = datetime.strptime(date_part, '%Y-%m-%d')
                    
                    if file_date < cutoff_date:
                        file_path.unlink()
                        logger.info(f"–£–¥–∞–ª–µ–Ω —Å—Ç–∞—Ä—ã–π —Ñ–∞–π–ª –Ω–æ–≤–æ—Å—Ç–µ–π: {file_path.name}")
                        
                except (ValueError, OSError) as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {file_path}: {e}")
            
            # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–π –∫–µ—à
            cache_manager.cleanup(max_age_days=7)
                    
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ —Å—Ç–∞—Ä—ã—Ö —Ñ–∞–π–ª–æ–≤: {e}")
    
    @monitor_performance("news_collection")
    @retry_with_exponential_backoff(max_attempts=3)
    @cache_api_response(ttl=300)  # –ö–µ—à–∏—Ä—É–µ–º –Ω–∞ 5 –º–∏–Ω—É—Ç
    def _collect_raw_news(self) -> Optional[str]:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç —Å—ã—Ä—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ Perplexity Deep Research
        
        Returns:
            str: –°—ã—Ä–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –æ—Ç API –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            prompt = get_perplexity_daily_collection_prompt()
            
            logger.info("üîç –ó–∞–ø—É—Å–∫–∞—é –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π...")
            logger.info("üìä –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –æ—Ç–≤–µ—Ç–∞: 8192 —Ç–æ–∫–µ–Ω–∞")
            
            payload = {
                "model": "sonar-deep-research",
                "messages": [
                    {
                        "role": "system",
                        "content": get_perplexity_system_prompt()
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                "max_tokens": PromptConfig.PERPLEXITY_COLLECTION_MAX_TOKENS,
                "temperature": PromptConfig.PERPLEXITY_TEMPERATURE,
                "top_p": PromptConfig.PERPLEXITY_TOP_P
            }
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º retry handler –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
            response = self.perplexity_retry.make_request(
                url=config.PERPLEXITY_API_URL,
                headers={
                    "Authorization": f"Bearer {config.PERPLEXITY_API_KEY}",
                    "Content-Type": "application/json"
                },
                json_data=payload,
                timeout=config.REQUEST_TIMEOUT
            )
            
            data = response.json()
            raw_content = data['choices'][0]['message']['content']
            
            logger.info("‚úÖ –£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç Perplexity Deep Research")
            logger.info(f"üìè –†–∞–∑–º–µ—Ä –æ—Ç–≤–µ—Ç–∞: {len(raw_content)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫—É
            metrics_collector.counters['news_collection_success'] += 1
            
            return raw_content
            
        except requests.exceptions.Timeout:
            logger.error("‚è∞ –ü—Ä–µ–≤—ã—à–µ–Ω —Ç–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –∫ Perplexity API")
            return None
        except requests.exceptions.RequestException as e:
            logger.error(f"üåê –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Perplexity API: {e}")
            return None
        except Exception as e:
            logger.error(f"üí• –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
            metrics_collector.record_error('news_collection', str(e))
            return None
    
    @cache_news_data(ttl=86400)  # –ö–µ—à–∏—Ä—É–µ–º –Ω–∞ 24 —á–∞—Å–∞
    def _process_raw_content(self, raw_content: str) -> List[Dict]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å—ã—Ä–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
        
        Args:
            raw_content: –°—ã—Ä–æ–π –æ—Ç–≤–µ—Ç –æ—Ç Deep Research
            
        Returns:
            List[Dict]: –°–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
        """
        try:
            # –û—á–∏—â–∞–µ–º –æ—Ç —Ç–µ–≥–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π
            if hasattr(self.perplexity_client, '_clean_deep_research_content'):
                cleaned_content = self.perplexity_client._clean_deep_research_content(raw_content)
            else:
                import re
                cleaned_content = re.sub(r'<think>.*?</think>', '', raw_content, flags=re.DOTALL)
                cleaned_content = re.sub(r'</?think>', '', cleaned_content)
            
            logger.debug(f"üìã –û—á–∏—â–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç:\n{cleaned_content[:500]}...")
            
            # –ü–∞—Ä—Å–∏–º –Ω–æ–≤–æ—Å—Ç–∏
            news_list = parse_collected_news(cleaned_content)
            
            logger.info(f"üì∞ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(news_list)} –Ω–æ–≤–æ—Å—Ç–µ–π")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫ –∫–∞–∂–¥–æ–π –Ω–æ–≤–æ—Å—Ç–∏
            current_time = now_msk()
            schedule = config.PUBLICATION_SCHEDULE
            
            for i, news_item in enumerate(news_list):
                # –ù–∞–∑–Ω–∞—á–∞–µ–º –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                if i < len(schedule):
                    news_item['scheduled_time'] = schedule[i]
                else:
                    # –ï—Å–ª–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –±–æ–ª—å—à–µ —á–µ–º –≤—Ä–µ–º–µ–Ω –≤ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–∏
                    news_item['scheduled_time'] = schedule[-1]  # –ü–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è
                
                # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                news_item.update({
                    'id': f"news_{current_time.strftime('%Y%m%d')}_{i+1}",
                    'collected_at': current_time.isoformat(),
                    'published': False,
                    'publication_attempts': 0
                })
            
            return news_list
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {e}")
            metrics_collector.record_error('content_processing', str(e))
            return []
    
    @monitor_performance("save_news_file")
    def _save_news_to_file(self, news_list: List[Dict], date: datetime) -> bool:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –≤ JSON —Ñ–∞–π–ª
        
        Args:
            news_list: –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            date: –î–∞—Ç–∞ –¥–ª—è –∫–æ—Ç–æ—Ä–æ–π —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –Ω–æ–≤–æ—Å—Ç–∏
            
        Returns:
            bool: True –µ—Å–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ—à–ª–æ —É—Å–ø–µ—à–Ω–æ
        """
        try:
            file_path = self._get_news_file_path(date)
            
            # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é –µ—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            if file_path.exists():
                create_backup(file_path)
            
            news_data = {
                'date': date.strftime('%Y-%m-%d'),
                'collected_at': now_msk().isoformat(),
                'total_news': len(news_list),
                'news': news_list
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å –±–ª–æ–∫–∏—Ä–æ–≤–∫–æ–π
            success = safe_json_write(file_path, news_data)
            
            if success:
                logger.info(f"üíæ –ù–æ–≤–æ—Å—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {file_path.name}")
                logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {len(news_list)} –Ω–æ–≤–æ—Å—Ç–µ–π")
                
                # –í—ã–≤–æ–¥–∏–º –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É
                for news in news_list:
                    priority = news.get('priority', 0)
                    title = news.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:50]
                    time = news.get('scheduled_time', '–ù–µ –Ω–∞–∑–Ω–∞—á–µ–Ω–æ')
                    logger.info(f"  üìå –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç {priority} ({time}): {title}...")
            
            return success
            
        except Exception as e:
            logger.error(f"üíæ –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
            metrics_collector.record_error('file_save', str(e))
            return False
    
    @monitor_performance("daily_collection")
    def collect_daily_news(self, target_date: Optional[datetime] = None) -> bool:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—É—é –¥–∞—Ç—É
        
        Args:
            target_date: –î–∞—Ç–∞ –¥–ª—è —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤—á–µ—Ä–∞)
            
        Returns:
            bool: True –µ—Å–ª–∏ —Å–±–æ—Ä –ø—Ä–æ—à–µ–ª —É—Å–ø–µ—à–Ω–æ
        """
        if target_date is None:
            target_date = yesterday_msk()
        
        logger.info("=" * 60)
        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ {format_date_russian(target_date)}")
        logger.info(f"üìä –°–∏—Å—Ç–µ–º–Ω–æ–µ –∑–¥–æ—Ä–æ–≤—å–µ: {metrics_collector.collect_system_metrics().cpu_percent:.1f}% CPU")
        logger.info("=" * 60)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å–æ–±–∏—Ä–∞–ª–∏ –ª–∏ —É–∂–µ –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ —ç—Ç—É –¥–∞—Ç—É
        file_path = self._get_news_file_path(target_date)
        if file_path.exists():
            logger.warning(f"‚ö†Ô∏è –§–∞–π–ª –Ω–æ–≤–æ—Å—Ç–µ–π —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {file_path.name}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à
            cached_data = safe_json_read(file_path)
            if cached_data and cached_data.get('total_news', 0) > 0:
                logger.info("üì¶ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ —Ñ–∞–π–ª–∞")
                return True
        
        # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ —Ñ–∞–π–ª—ã
        self._cleanup_old_files()
        
        # –ü—ã—Ç–∞–µ–º—Å—è —Å–æ–±—Ä–∞—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
        for attempt in range(1, self.max_retries + 1):
            logger.info(f"üéØ –ü–æ–ø—ã—Ç–∫–∞ —Å–±–æ—Ä–∞ #{attempt}/{self.max_retries}")
            
            # –°–æ–±–∏—Ä–∞–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ
            raw_content = self._collect_raw_news()
            if not raw_content:
                if attempt < self.max_retries:
                    logger.info(f"‚è±Ô∏è –û–∂–∏–¥–∞–Ω–∏–µ {self.retry_delay} —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
                    time.sleep(self.retry_delay)
                continue
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            news_list = self._process_raw_content(raw_content)
            if not news_list:
                logger.warning("üì∞ –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –æ—Ç–≤–µ—Ç–∞")
                if attempt < self.max_retries:
                    logger.info(f"‚è±Ô∏è –û–∂–∏–¥–∞–Ω–∏–µ {self.retry_delay} —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
                    time.sleep(self.retry_delay)
                continue
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –¥–ª—è –≤—Å–µ—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
            logger.info("üé® –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
            news_list_with_images = self._generate_images_for_news(news_list, target_date)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
            if self._save_news_to_file(news_list_with_images, target_date):
                logger.info("üéâ –°–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
                metrics_collector.save_metrics()
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                stats = metrics_collector.generate_daily_stats()
                logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–Ω—è: {stats.news_collected} –Ω–æ–≤–æ—Å—Ç–µ–π —Å–æ–±—Ä–∞–Ω–æ")
                
                logger.info("=" * 60)
                return True
            else:
                logger.error("üíæ –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞")
                if attempt < self.max_retries:
                    time.sleep(self.retry_delay)
                continue
        
        logger.error("‚ùå –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ —Å–±–æ—Ä–∞ –∏—Å—á–µ—Ä–ø–∞–Ω—ã")
        metrics_collector.counters['news_collection_failed'] += 1
        logger.info("=" * 60)
        return False
    
    def get_news_file_status(self, date: Optional[datetime] = None) -> Dict:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç—É—Å —Ñ–∞–π–ª–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—É—é –¥–∞—Ç—É
        
        Args:
            date: –î–∞—Ç–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤—á–µ—Ä–∞)
            
        Returns:
            Dict: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç–∞—Ç—É—Å–µ —Ñ–∞–π–ª–∞
        """
        if date is None:
            date = yesterday_msk()
        
        file_path = self._get_news_file_path(date)
        
        if not file_path.exists():
            return {
                'exists': False,
                'date': date.strftime('%Y-%m-%d'),
                'file_path': str(file_path)
            }
        
        try:
            data = safe_json_read(file_path)
            
            if data:
                return {
                    'exists': True,
                    'date': date.strftime('%Y-%m-%d'),
                    'file_path': str(file_path),
                    'collected_at': data.get('collected_at'),
                    'total_news': data.get('total_news', 0),
                    'news_count': len(data.get('news', [])),
                    'published_count': sum(1 for news in data.get('news', []) if news.get('published', False))
                }
            else:
                return {
                    'exists': True,
                    'date': date.strftime('%Y-%m-%d'),
                    'file_path': str(file_path),
                    'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª'
                }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")
            return {
                'exists': True,
                'date': date.strftime('%Y-%m-%d'),
                'file_path': str(file_path),
                'error': str(e)
            }


def main():
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥—É–ª—è"""
    logger.info("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ NewsCollector...")
    
    collector = NewsCollector()
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π
    success = collector.collect_daily_news()
    
    if success:
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç—É—Å
        status = collector.get_news_file_status()
        logger.info(f"üìä –°—Ç–∞—Ç—É—Å: {status}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        summary = metrics_collector.get_summary()
        logger.info(f"üìà –ú–µ—Ç—Ä–∏–∫–∏: {summary}")
    else:
        logger.error("‚ùå –¢–µ—Å—Ç –Ω–µ –ø—Ä–æ—à–µ–ª")


if __name__ == "__main__":
    main()